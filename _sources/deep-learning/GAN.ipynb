{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db81b846",
   "metadata": {},
   "source": [
    "# Generative adversarial networks \n",
    "\n",
    "The original purpose of Generative adversarial networks(GAN) is to generate new data. It classically generates new images, but is applicable to wide range of domains. It learns the training set distribution and can generate new images that have never been seen before. In contrast to e.g., autoregressive models or RNNs (generating one word at a time), GANs generate the whole output all at once.\n",
    "\n",
    "```{seealso}\n",
    "GAN is proposed in 2005, the paper is Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. \"Generative Adversarial Networks\", arxiv:1406.2661.\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "The structures of GANs are roughly as follows. They have two essential part, discriminator and generator. \n",
    "Discriminator learns to become better at distinguishing real from generated images, and generator learns to generate better images to fool the discriminator.\n",
    "\n",
    ":::{figure-md} GAN structure\n",
    "<img src=\"../../images/deep-learning/GAN/structure.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of GAN structure\n",
    ":::\n",
    "\n",
    "Then, one question appears: why are GANs Called Generative Models?\n",
    "- The generative part comes from the fact that the model \"generates\" new data,\n",
    "- Most of the time, generative models use an approximation to compute the usually intractable distribution; here, the discriminator part does that approximation,\n",
    "- So, it does learn p(x).\n",
    "\n",
    "## Training\n",
    "\n",
    "To make the model converge, we need to find a proper objective for it.\n",
    "For the GAN, it is $\\min\\limits_{G} \\max\\limits_{D} V(D, G) = \\mathbb{E}_{x \\thicksim p_{data}}[log D(x)] + \\mathbb{E}_{z\\thicksim p_z(z)}[log(1-D(G(z)))]$\n",
    "\n",
    "### Discriminator gradient for update (gradient ascent)\n",
    "\n",
    "The aim of Discriminator is $\\bigtriangledown_{W_D} \\frac{1}{n} \\sum_{i=1}^n [log D(x^{(i)}) + log (1-D(G(z^{(i)})))]$. We can split it into two parts.\n",
    "First is $D(x^{(i)})$. If it predicts well in real images, the probability will be close to $1$. \n",
    "Second is $D(G(z^{(i)}))$. This part use to predict the fake images which generdated from the generator, and if it predicts well, the probability will be close to $0$.\n",
    "\n",
    "### Generator gradient for update (gradient descent)\n",
    "\n",
    "The aim of Generator is $\\bigtriangledown_{W_G} \\frac{1}{n} \\sum_{i=1}^n log(1-D(G(z^{(i)})))$. \n",
    "$D(G(z^{(i)}))$ is also used to predict fake images, but if it predicts badly on fake images, the probability will be close to 1, which is not same to discriminator.\n",
    "\n",
    "### Train for convergence\n",
    "\n",
    "GAN converges when probabilities are close to 0.5, which means G can cheat D.\n",
    "rom a mathematical point of view, it converges when Nash-equilibrium (Game Theory concept) is reached in the minmax (zero-sum) game.\n",
    "[Nash-Equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) in Game Theory is reached when the actions of one player won't change depending on the opponent's actions.\n",
    "\n",
    "But not each time GAN can converge, there are some training problems:\n",
    "- Oscillation between generator and discriminator loss,\n",
    "- Mode collapse (generator produces examples of a particular kind only),\n",
    "- Discriminator is too strong, such that the gradient for the generator vanishes and the generator can't keep up,\n",
    "- Discriminator is too weak, and the generator produces non-realistic images that fool it too easily (rare problem, though).\n",
    "\n",
    "For the third problem, replacing $\\bigtriangledown_{W_G} \\frac{1}{n} \\sum_{i=1}^n log(1-D(G(z^{(i)})))$ with $\\bigtriangledown_{W_G} \\frac{1}{n} \\sum_{i=1}^n log(D(G(z^{(i)})))$ can be a good choice.\n",
    "\n",
    "### Loss\n",
    "\n",
    "Discriminator:\n",
    "- Maximize prediction probability of classifying real as real and fake as fake,\n",
    "- Remember maximizing log likelihood is the same as minimizing negative log likelihood (i.e., minimizing cross-entropy).\n",
    "Generator:\n",
    "- Minimize likelihood of the discriminator to make correct predictions (predict fake as fake; real as real), which can be achieved by maximizing the cross-entropy,\n",
    "- This doesn't work well in practice though because of gradient issues (zero gradient if the discriminator makes correct predictions, which is not what we want for the generator),\n",
    "- Better: flip labels and minimize cross entropy (force the discriminator to output high probability for fake if an image is real, and high probability for real if an image is fake).\n",
    "\n",
    "### Code\n",
    "\n",
    "Here we implement a GAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3747e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from utils import Logger\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist=input_data.read_data_sets('./tf_data/VGAN/MNIST',one_hot=True)\n",
    "DATA_FOLDER = './tf_data/VGAN/MNIST'\n",
    "IMAGE_PIXELS = 28*28\n",
    "NOISE_SIZE = 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def noise(n_rows, n_cols):\n",
    "    return np.random.normal(size=(n_rows, n_cols))\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0] if len(size) == 1 else size[1]\n",
    "    stddev = 1. / np.sqrt(float(in_dim))\n",
    "    return tf.random_uniform(shape=size, minval=-stddev, maxval=stddev)\n",
    "\n",
    "def images_to_vectors(images):\n",
    "    return images.reshape(images.shape[0], 784)\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.reshape(vectors.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328e604",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((.5,), (.5,))\n",
    "        ])\n",
    "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "\n",
    "# Load data\n",
    "data = mnist_data()\n",
    "# Create loader with data, so that we can iterate over it\n",
    "data_loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# Num batches\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986e81b",
   "metadata": {},
   "source": [
    "Initialize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c34616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discriminator\n",
    "\n",
    "# Input\n",
    "X = tf.placeholder(tf.float32, shape=(None, IMAGE_PIXELS))\n",
    "\n",
    "# Layer 1 Variables\n",
    "D_W1 = tf.Variable(xavier_init([784, 1024]))\n",
    "D_B1 = tf.Variable(xavier_init([1024]))\n",
    "\n",
    "# Layer 2 Variables\n",
    "D_W2 = tf.Variable(xavier_init([1024, 512]))\n",
    "D_B2 = tf.Variable(xavier_init([512]))\n",
    "\n",
    "# Layer 3 Variables\n",
    "D_W3 = tf.Variable(xavier_init([512, 256]))\n",
    "D_B3 = tf.Variable(xavier_init([256]))\n",
    "\n",
    "# Out Layer Variables\n",
    "D_W4 = tf.Variable(xavier_init([256, 1]))\n",
    "D_B4 = tf.Variable(xavier_init([1]))\n",
    "\n",
    "# Store Variables in list\n",
    "D_var_list = [D_W1, D_B1, D_W2, D_B2, D_W3, D_B3, D_W4, D_B4]\n",
    "\n",
    "## Generator\n",
    "\n",
    "# Input\n",
    "Z = tf.placeholder(tf.float32, shape=(None, NOISE_SIZE))\n",
    "\n",
    "# Layer 1 Variables\n",
    "G_W1 = tf.Variable(xavier_init([100, 256]))\n",
    "G_B1 = tf.Variable(xavier_init([256]))\n",
    "\n",
    "# Layer 2 Variables\n",
    "G_W2 = tf.Variable(xavier_init([256, 512]))\n",
    "G_B2 = tf.Variable(xavier_init([512]))\n",
    "\n",
    "# Layer 3 Variables\n",
    "G_W3 = tf.Variable(xavier_init([512, 1024]))\n",
    "G_B3 = tf.Variable(xavier_init([1024]))\n",
    "\n",
    "# Out Layer Variables\n",
    "G_W4 = tf.Variable(xavier_init([1024, 784]))\n",
    "G_B4 = tf.Variable(xavier_init([784]))\n",
    "\n",
    "# Store Variables in list\n",
    "G_var_list = [G_W1, G_B1, G_W2, G_B2, G_W3, G_B3, G_W4, G_B4]\n",
    "\n",
    "def discriminator(x):\n",
    "    l1 = tf.nn.dropout(tf.nn.leaky_relu(tf.matmul(x,   D_W1) + D_B1, .2), .3)\n",
    "    l2 = tf.nn.dropout(tf.nn.leaky_relu(tf.matmul(l1,  D_W2) + D_B2, .2), .3)\n",
    "    l3 = tf.nn.dropout(tf.nn.leaky_relu(tf.matmul(l2,  D_W3) + D_B3, .2), .3)\n",
    "    out = tf.matmul(l3, D_W4) + D_B4\n",
    "    return out\n",
    "\n",
    "def generator(z):\n",
    "    l1 = tf.nn.leaky_relu(tf.matmul(z,  G_W1) + G_B1, .2)\n",
    "    l2 = tf.nn.leaky_relu(tf.matmul(l1, G_W2) + G_B2, .2)\n",
    "    l3 = tf.nn.leaky_relu(tf.matmul(l2, G_W3) + G_B3, .2)\n",
    "    out = tf.nn.tanh(tf.matmul(l3, G_W4) + G_B4)\n",
    "    return out\n",
    "    \n",
    "G_sample = generator(Z)\n",
    "D_real = discriminator(X)\n",
    "D_fake = discriminator(G_sample)\n",
    "\n",
    "# Losses\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, labels=tf.ones_like(D_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.zeros_like(D_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "# Optimizers\n",
    "D_opt = tf.train.AdamOptimizer(2e-4).minimize(D_loss, var_list=D_var_list)\n",
    "G_opt = tf.train.AdamOptimizer(2e-4).minimize(G_loss, var_list=G_var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e94b2",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples, NOISE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599a880",
   "metadata": {},
   "source": [
    "Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ece683",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "# Start interactive session\n",
    "session = tf.InteractiveSession()\n",
    "# Init Variables\n",
    "tf.global_variables_initializer().run()\n",
    "# Init Logger\n",
    "logger = Logger(model_name='DCGAN1', data_name='CIFAR10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258928b3",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, (batch,_) in enumerate(data_loader):\n",
    "        \n",
    "        # 1. Train Discriminator\n",
    "        X_batch = images_to_vectors(batch.permute(0, 2, 3, 1).numpy())\n",
    "        feed_dict = {X: X_batch, Z: noise(BATCH_SIZE, NOISE_SIZE)}\n",
    "        _, d_error, d_pred_real, d_pred_fake = session.run(\n",
    "            [D_opt, D_loss, D_real, D_fake], feed_dict=feed_dict\n",
    "        )\n",
    "\n",
    "        # 2. Train Generator\n",
    "        feed_dict = {Z: noise(BATCH_SIZE, NOISE_SIZE)}\n",
    "        _, g_error = session.run(\n",
    "            [G_opt, G_loss], feed_dict=feed_dict\n",
    "        )\n",
    "\n",
    "        if n_batch % 100 == 0:\n",
    "            display.clear_output(True)\n",
    "            # Generate images from test noise\n",
    "            test_images = session.run(\n",
    "                G_sample, feed_dict={Z: test_noise}\n",
    "            )\n",
    "            test_images = vectors_to_images(test_images)\n",
    "            # Log Images\n",
    "            logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches, format='NHWC');\n",
    "            # Log Status\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, n_batch, num_batches,\n",
    "                d_error, g_error, d_pred_real, d_pred_fake\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caad06d",
   "metadata": {},
   "source": [
    "## Your turn! 🚀\n",
    "\n",
    "TBD.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Sebastian Raschka](https://github.com/rasbt) for creating the open-source project [stat453-deep-learning-ss20](https://github.com/rasbt/stat453-deep-learning-ss20) and [Diego Gomez](https://github.com/diegoalejogm) for creating the open-source project [gans](https://github.com/diegoalejogm/gans). They inspire the majority of the content in this chapter.\n",
    "\n",
    "---\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   84,
   118,
   122,
   137,
   141,
   217,
   221,
   224,
   228,
   237,
   241,
   273
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
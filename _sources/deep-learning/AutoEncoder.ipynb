{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c78a7e7",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "## Overview\n",
    "\n",
    "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Autoencoder is a kind of unsupervised learning, which means working with datasets without considering a target variable. There are some Applications and Goals for it:\n",
    "\n",
    "- Finding hidden structures in data.\n",
    "- Data compression.\n",
    "- Clustering.\n",
    "- Retrieving similar objects.\n",
    "- Exploratory data analysis.\n",
    "- Generating new examples.\n",
    "\n",
    "And for unsupervised learning, its main Principal Component Analysis (PCA) is:\n",
    "\n",
    "- Find directions of maximum variance\n",
    "\n",
    ":::{figure-md} 01_PCA1\n",
    "<img src=\"../../images/deep-learning/autoencoder/01_PCA1.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of PCA\n",
    ":::\n",
    "\n",
    "- Transform features onto directions of maximum variance\n",
    "\n",
    ":::{figure-md} 02_PCA2\n",
    "<img src=\"../../images/deep-learning/autoencoder/02_PCA2.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of PCA\n",
    ":::\n",
    "\n",
    "- Usually consider a subset of vectors of most variance (dimensionality reduction)\n",
    "\n",
    ":::{figure-md} 03_PCA3\n",
    "<img src=\"../../images/deep-learning/autoencoder/03_PCA3.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of PCA\n",
    ":::\n",
    "\n",
    "## Fully-connected Autoencoder\n",
    "\n",
    "Here is an example of a basic fully-connected autoencoder\n",
    "\n",
    ":::{figure-md} 04_simple\n",
    "<img src=\"../../images/deep-learning/autoencoder/04_simple.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of Fully Connected autoencoder\n",
    ":::\n",
    "\n",
    "```{note}\n",
    "If we don't use non-linear activation functions and minimize the MSE, this is very similar to PCA. However, the latent dimensions will not necessarily be orthogonal and will have same variance.\n",
    "```\n",
    "\n",
    "The loss function of this simple model is $L(x, x^') = ||x - x^'||^2_2 = \\sum_i (x_i - x_i^')^2$.\n",
    "\n",
    "### Potential Autoencoder Applications\n",
    "\n",
    "And there are some potential autoencoder applications, for example:\n",
    "- After training, disregard the output part, we can use embedding as input to classic machine learning methods (SVM, KNN, Random Forest, ...).\n",
    "- Similar to transfer learning, we can train autoencoder on large image dataset, then fine tune encoder part on your own, smaller dataset and/or provide your own output (classification) layer.\n",
    "- Latent space can also be used for visualization (EDA, clustering), but there are better methods for that.\n",
    "\n",
    "## Convolutional Autoencoder\n",
    "\n",
    "For convolutional autoencoder, we mainly use transposed convolution construct the output, and transposed convolution (sometimes called \"deconvolution\") allows us to increase the size of the output feature map compared to the input feature map.\n",
    "\n",
    "The difference between regular convolution and transposed convolution can be seen from the following image.\n",
    "\n",
    ":::{figure-md} 05_diff_conv\n",
    "<img src=\"../../images/deep-learning/autoencoder/05_diff_conv.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Difference between regular and transposed convolution\n",
    ":::\n",
    "\n",
    "In transposed convolutions, we stride over the output; hence, larger strides will result in larger outputs (opposite to regular convolutions); and we pad the output; hence, larger padding will result in smaller output maps.\n",
    "\n",
    "So, the whole model consists of two parts, encoder and decoder, and they are composed with regular convolution and transposed convolution respectively.\n",
    "\n",
    ":::{figure-md} 06_convmodel\n",
    "<img src=\"../../images/deep-learning/autoencoder/06_convmodel.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of convoluted autoencoder\n",
    ":::\n",
    "\n",
    "```{note}\n",
    "Here is some other tricks to help our training:\n",
    "1. Add dropout layers to force networks to learn redundant features.\n",
    "2. Add dropout after the input, or add noise to the input to learn to denoise images.\n",
    "3. Add L1 penalty to the loss to learn sparse feature representations.\n",
    "```\n",
    "\n",
    "## Code\n",
    "\n",
    "Let's build a 2-layers auto-encoder with TensorFlow to compress images to a lower latent space and then reconstruct them. And this project will be done on MNIST dataste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee6683",
   "metadata": {},
   "source": [
    "MNIST Dataset parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062daf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 784 # data features (img shape: 28*28)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db882437",
   "metadata": {},
   "source": [
    "Training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096541ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_steps = 20000\n",
    "batch_size = 256\n",
    "display_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957519f",
   "metadata": {},
   "source": [
    "Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_1 = 128 # 1st layer num features.\n",
    "num_hidden_2 = 64 # 2nd layer num features (the latent dim)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f90f4",
   "metadata": {},
   "source": [
    "Prepare MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Convert to float32.\n",
    "x_train, x_test = x_train.astype(np.float32), x_test.astype(np.float32)\n",
    "# Flatten images to 1-D vector of 784 features (28*28).\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a942b7",
   "metadata": {},
   "source": [
    "Store layers weight & bias.\n",
    "A random value generator to initialize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3cbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal = tf.initializers.RandomNormal()\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(random_normal([num_features, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(random_normal([num_hidden_1, num_features])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(random_normal([num_features])),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184db1c",
   "metadata": {},
   "source": [
    "Building the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e840af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation.\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation.\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61be299",
   "metadata": {},
   "source": [
    "Building the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation.\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation.\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70d8f8",
   "metadata": {},
   "source": [
    "Mean square loss between original images and reconstructed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square(reconstructed, original):\n",
    "    return tf.reduce_mean(tf.pow(original - reconstructed, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06605d",
   "metadata": {},
   "source": [
    "Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca36d8",
   "metadata": {},
   "source": [
    "Optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d428bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(x):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        reconstructed_image = decoder(encoder(x))\n",
    "        loss = mean_square(reconstructed_image, x)\n",
    "\n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = weights.values() + biases.values()\n",
    "    \n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eae69b",
   "metadata": {},
   "source": [
    "Run training for the given number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (batch_x, _) in enumerate(train_data.take(training_steps + 1)):\n",
    "    \n",
    "    # Run the optimization.\n",
    "    loss = run_optimization(batch_x)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        print(\"step: %i, loss: %f\" % (step, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f0b60",
   "metadata": {},
   "source": [
    "Testing and Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c580ff",
   "metadata": {},
   "source": [
    "Encode and decode images from test set and visualize their reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "canvas_orig = np.empty((28 * n, 28 * n))\n",
    "canvas_recon = np.empty((28 * n, 28 * n))\n",
    "for i, (batch_x, _) in enumerate(test_data.take(n)):\n",
    "    # Encode and decode the digit image.\n",
    "    reconstructed_images = decoder(encoder(batch_x))\n",
    "    # Display original images.\n",
    "    for j in range(n):\n",
    "        # Draw the generated digits.\n",
    "        img = batch_x[j].numpy().reshape([28, 28])\n",
    "        canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = img\n",
    "    # Display reconstructed images.\n",
    "    for j in range(n):\n",
    "        # Draw the generated digits.\n",
    "        reconstr_img = reconstructed_images[j].numpy().reshape([28, 28])\n",
    "        canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = reconstr_img\n",
    "\n",
    "print(\"Original Images\")     \n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstructed Images\")\n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640d51c",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "\n",
    "https://github.com/reveurmichael/anomagram\n",
    "\n",
    "https://anomagram.fastforwardlabs.com/#/\n",
    "\n",
    "https://medium.datadriveninvestor.com/deep-learning-autoencoders-db265359943e\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Your turn! 🚀\n",
    "\n",
    "TBD.\n",
    "\n",
    "## Self study\n",
    "\n",
    "You can refer to this book chapter for further study:\n",
    "\n",
    "- [deeplearningbook](https://www.deeplearningbook.org/contents/autoencoders.html)\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Sebastian Raschka](https://github.com/rasbt) for creating the open-source project [stat453-deep-learning-ss20](https://github.com/rasbt/stat453-deep-learning-ss20) and [Aymeric Damien](https://github.com/aymericdamien) for creating the open-source project [TensorFlow-Examples](https://github.com/aymericdamien/TensorFlow-Examples/). They inspire the majority of the content in this chapter.\n",
    "\n",
    "---\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   115,
   118,
   122,
   124,
   128,
   133,
   137,
   140,
   144,
   153,
   158,
   173,
   177,
   186,
   190,
   199,
   203,
   206,
   210,
   212,
   217,
   234,
   238,
   246,
   250,
   252,
   256,
   283
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
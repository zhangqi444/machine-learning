{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c003195",
   "metadata": {},
   "source": [
    "# Long-short term memory\n",
    "\n",
    "As we have learned the normal RNNs on the previous lessons, today we are going to use long-short term memory (LSTM) to fix the problems that are induced from the RNN architecture.\n",
    "\n",
    "```{seealso}\n",
    "The vanilla LSTM is proposed in 2005, the paper is [Bidirectional LSTM Networks for Improved Phoneme Classification and Recognition](https://link.springer.com/chapter/10.1007/11550907_126), and after that, a lot of paper based on it appeared.\n",
    "```\n",
    "\n",
    "## Overview\n",
    "The essential problems of RNN are vanishing/exploding gradient problems.The gradient vanishing/exploding problem is a common issue in training deep neural networks. This problem occurs when the gradient of the loss function with respect to the weights of the network becomes very small or very large.\n",
    "\n",
    "There are several possibilities will bring on this kind of problem, for example:\n",
    "- a poor choice of hyper-parameters,\n",
    "- a poor architecture,\n",
    "- a bug in the code.\n",
    "\n",
    "Luckily, LSTM can use a memory cell for modeling long-range dependencies and avoid vanishing/exploding gradient problems.\n",
    "\n",
    "First, let's see the architecture of LSTM cell.\n",
    "\n",
    ":::{figure-md} LSTM cell\n",
    "<img src=\"../../images/deep-learning/LSTM/LSTM_cell.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of LSTM unit {cite}`LSTM_cell`\n",
    ":::\n",
    "\n",
    "It looks like the cell of RNN, but there are still some differences between them. The same part is the direction of data, one input and two outputs. \n",
    "However, the LSTM cell adds a cell state $c^{<t>}$, which updates with the time. $c^{<t-1>}$ means cell state at previous time step and $c^{<t-1>}$ means cell state at current time step.\n",
    "The $h^{t}$ is still the activation.\n",
    "\n",
    ":::{figure-md} LSTM cell state\n",
    "<img src=\"../../images/deep-learning/LSTM/cell_state.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "LSTM unit of state c {cite}`LSTM_cell_state`\n",
    ":::\n",
    "\n",
    "## Special Gates\n",
    "Then, we take a look at the inside of cell. $\\bigodot$ means element-wise multiplication operator, $\\bigoplus$ means element-wise addition operator and $\\sigma$ means logistic sigmoid activation functions.\n",
    "\n",
    "Now, we should pay attention to 'Gates'. \n",
    "\n",
    "The first is 'Forget Gate' $f$, and it controls which information is remembered, and which is forgotten; can reset the cell state. This function can be written as $f_t = \\sigma (W_{fx}x^{<t>} + W_{fh}h^{<t-1>} + b_f)$\n",
    "\n",
    "Next, 'Input Gate' is $i_t = \\sigma(W_{ix}x^{<t>} + W_{ih}h^{<t-1>} + b_i)$ and 'Input Node' is $g_t = tanh(W_{gt}x^{<t>} + W_{gh}x^{<t-1>} + b_g)$.\n",
    "\n",
    "To brief summarize the previous gates in an expression: $C^{<t>} = (C^{<t-1>} \\bigodot f_t) \\bigoplus (i_t \\bigodot g_t)$. Since $i_t$ is 'Input Gate' and $g_t$ is 'Input Gate', $(i_t \\bigodot g_t)$ is for updatting the cell state.\n",
    "\n",
    "Finally, 'Output Gate' is for updating the values of hidden units: $o_t = \\sigma(W_{ox}x^{<t>} + W_{oh}x^{<t-1>} + b_o)$. So the activateion of the current time is $h^{<t>} = o_t \\bigodot tanh(C^{<t>})$.\n",
    "\n",
    "### Code\n",
    "Here we implement an LSTM model on all a data set of Shakespeare works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1107993",
   "metadata": {},
   "source": [
    "Start a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1178a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b740f",
   "metadata": {},
   "source": [
    "Set RNN Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_freq = 5  # Trim the less frequent words off\n",
    "rnn_size = 128  # RNN Model size\n",
    "epochs = 10  # Number of epochs to cycle through data\n",
    "batch_size = 100  # Train on this many examples at once\n",
    "learning_rate = 0.001  # Learning rate\n",
    "training_seq_len = 50  # how long of a word group to consider\n",
    "embedding_size = rnn_size  # Word embedding size\n",
    "save_every = 500  # How often to save model checkpoints\n",
    "eval_every = 50  # How often to evaluate the test sentences\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd8ab5",
   "metadata": {},
   "source": [
    "Download/store Shakespeare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89103bae",
   "metadata": {},
   "source": [
    "Declare punctuation to remove, everything except hyphens and apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef2ff0",
   "metadata": {},
   "source": [
    "Make Model Directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae779df",
   "metadata": {},
   "source": [
    "Make data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4201aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "print('Loading Shakespeare Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f7923",
   "metadata": {},
   "source": [
    "Check if file is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40145772",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs.\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    # If file has been saved, load from that file\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c88d7",
   "metadata": {},
   "source": [
    "Clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning Text')\n",
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text).strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407523a",
   "metadata": {},
   "source": [
    "Build word vocabulary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ec251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text, min_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    # limit word counts to those more frequent than cutoff\n",
    "    word_counts = {key: val for key, val in word_counts.items() if val > min_freq}\n",
    "    # Create vocab --> index mapping\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key: (i_x+1) for i_x, key in enumerate(words)}\n",
    "    # Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unknown'] = 0\n",
    "    # Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val: key for key, val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return ix_to_vocab_dict, vocab_to_ix_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e31e3",
   "metadata": {},
   "source": [
    "Build Shakespeare vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building Shakespeare Vocab')\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1\n",
    "print('Vocabulary Length = {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142e9b7",
   "metadata": {},
   "source": [
    "Sanity Check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(ix2vocab) == len(vocab2ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a3092",
   "metadata": {},
   "source": [
    "Convert text to word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ddcb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except KeyError:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ede626",
   "metadata": {},
   "source": [
    "Define LSTM RNN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            out = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return out\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output], [tf.reshape(self.y_output, [-1])],\n",
    "                        [tf.ones([self.batch_size * self.training_seq_len])])\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return out_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313877d",
   "metadata": {},
   "source": [
    "Define LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                        training_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0802a",
   "metadata": {},
   "source": [
    "Tell TensorFlow we are reusing the scope for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee342a6",
   "metadata": {},
   "source": [
    "Create model saver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1e4ee",
   "metadata": {},
   "source": [
    "Create batches for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89793cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f42e91",
   "metadata": {},
   "source": [
    "Split up text indices into subarrays, of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = np.array_split(s_text_ix, num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ea98a",
   "metadata": {},
   "source": [
    "Reshape each split into [batch_size, training_seq_len]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3a8b2",
   "metadata": {},
   "source": [
    "Initialize all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76188c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ccf59",
   "metadata": {},
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run a through one epoch\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    # Reset initial LSTM state every epoch\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            # Save model\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step=iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # Save vocabulary\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b6f98",
   "metadata": {},
   "source": [
    "Plot loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38aadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcac3b",
   "metadata": {},
   "source": [
    "## Stacking LSTM Layers\n",
    "\n",
    "As we did before in RNN chapter, we can stack simple LSTM layers into a more complex model.\n",
    "\n",
    "### Code\n",
    "\n",
    "Here we implement an LSTM model on all a data set of Shakespeare works.\n",
    "We will stack multiple LSTM models for a more accurate representation of Shakespearean language. We will also use characters instead of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b79745",
   "metadata": {},
   "source": [
    "Start a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91925746",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed883d61",
   "metadata": {},
   "source": [
    "Set RNN Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39059cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 3  # Number of RNN layers stacked\n",
    "min_word_freq = 5  # Trim the less frequent words off\n",
    "rnn_size = 128  # RNN Model size, has to equal embedding size\n",
    "epochs = 10  # Number of epochs to cycle through data\n",
    "batch_size = 100  # Train on this many examples at once\n",
    "learning_rate = 0.0005  # Learning rate\n",
    "training_seq_len = 50  # how long of a word group to consider\n",
    "save_every = 500  # How often to save model checkpoints\n",
    "eval_every = 50  # How often to evaluate the test sentences\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc52fa",
   "metadata": {},
   "source": [
    "Download/store Shakespeare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5264ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab6067",
   "metadata": {},
   "source": [
    "Declare punctuation to remove, everything except hyphens and apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28899303",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bae574",
   "metadata": {},
   "source": [
    "Make Model Directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f87bcc",
   "metadata": {},
   "source": [
    "Make data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print('Loading Shakespeare Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36274c95",
   "metadata": {},
   "source": [
    "Check if file is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76daeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs.\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    # If file has been saved, load from that file\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8074b2",
   "metadata": {},
   "source": [
    "Clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec262ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning Text')\n",
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text).strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048faef4",
   "metadata": {},
   "source": [
    "Split up by characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eded398",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list = list(s_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4a5de",
   "metadata": {},
   "source": [
    "Build word vocabulary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(characters):\n",
    "    character_counts = collections.Counter(characters)\n",
    "    # Create vocab --> index mapping\n",
    "    chars = character_counts.keys()\n",
    "    vocab_to_ix_dict = {key: (inx + 1) for inx, key in enumerate(chars)}\n",
    "    # Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unknown'] = 0\n",
    "    # Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val: key for key, val in vocab_to_ix_dict.items()}\n",
    "    return ix_to_vocab_dict, vocab_to_ix_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186e427",
   "metadata": {},
   "source": [
    "Build Shakespeare vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15338f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building Shakespeare Vocab by Characters')\n",
    "ix2vocab, vocab2ix = build_vocab(char_list)\n",
    "vocab_size = len(ix2vocab)\n",
    "print('Vocabulary Length = {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86bf2f3",
   "metadata": {},
   "source": [
    "Sanity Check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(ix2vocab) == len(vocab2ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffa64f",
   "metadata": {},
   "source": [
    "Convert text to word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text_ix = []\n",
    "for x in char_list:\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except KeyError:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc340c",
   "metadata": {},
   "source": [
    "Define LSTM RNN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8515dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self, rnn_size, num_layers, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        self.lstm_cell = tf.contrib.rnn.MultiRNNCell([self.lstm_cell for _ in range(self.num_layers)])\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell)\n",
    "        \n",
    "        # RNN outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=20, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        char_list = list(prime_text)\n",
    "        for char in char_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        char = char_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            char = words[sample]\n",
    "            out_sentence = out_sentence + char\n",
    "        return out_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9962efa",
   "metadata": {},
   "source": [
    "Define LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_Model(rnn_size,\n",
    "                        num_layers,\n",
    "                        batch_size,\n",
    "                        learning_rate,\n",
    "                        training_seq_len,\n",
    "                        vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c0a1c",
   "metadata": {},
   "source": [
    "Tell TensorFlow we are reusing the scope for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9818246",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(rnn_size,\n",
    "                                 num_layers,\n",
    "                                 batch_size,\n",
    "                                 learning_rate,\n",
    "                                 training_seq_len,\n",
    "                                 vocab_size,\n",
    "                                 infer_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53823f59",
   "metadata": {},
   "source": [
    "Create model saver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e656c5",
   "metadata": {},
   "source": [
    "Create batches for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ebfad",
   "metadata": {},
   "source": [
    "Split up text indices into subarrays, of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343cfddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = np.array_split(s_text_ix, num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f5e4e",
   "metadata": {},
   "source": [
    "Reshape each split into [batch_size, training_seq_len]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ec428",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbd66e",
   "metadata": {},
   "source": [
    "Initialize all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b02ef6",
   "metadata": {},
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee1cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run a through one epoch\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    # Reset initial LSTM state every epoch\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        # We need to update initial state for each RNN cell:\n",
    "        for i, (c, h) in enumerate(lstm_model.initial_state):\n",
    "                    training_dict[c] = state[i].c\n",
    "                    training_dict[h] = state[i].h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            # Save model\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step=iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # Save vocabulary\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92457d",
   "metadata": {},
   "source": [
    "Plot loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd4caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce94b2",
   "metadata": {},
   "source": [
    "## Your turn! 🚀\n",
    "\n",
    "Practice the Long-Short Term Memory Networks by following this TBD.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Nick](https://github.com/nfmcclure) for creating the open-source project [tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook) and [Sebastian Raschka](https://github.com/rasbt) for creating the open-source project [stat453-deep-learning-ss20](https://github.com/rasbt/stat453-deep-learning-ss20). They inspire the majority of the content in this chapter.\n",
    "\n",
    "---\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   68,
   81,
   84,
   86,
   89,
   100,
   103,
   108,
   111,
   114,
   117,
   120,
   123,
   127,
   130,
   152,
   155,
   159,
   162,
   176,
   179,
   184,
   187,
   189,
   192,
   201,
   204,
   293,
   296,
   299,
   302,
   306,
   309,
   311,
   314,
   316,
   319,
   321,
   324,
   326,
   329,
   332,
   335,
   378,
   381,
   387,
   398,
   411,
   414,
   416,
   419,
   430,
   433,
   438,
   441,
   444,
   447,
   450,
   453,
   457,
   460,
   482,
   485,
   489,
   492,
   494,
   497,
   508,
   511,
   516,
   519,
   521,
   524,
   532,
   535,
   615,
   618,
   625,
   628,
   637,
   640,
   642,
   645,
   647,
   650,
   652,
   655,
   657,
   660,
   663,
   666,
   710,
   713,
   719
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
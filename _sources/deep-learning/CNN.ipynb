{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d5fc8e",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "```{epigraph}\n",
    "Thanks to Convolutional Neural Network, computer vision is working far better than just two years ago, and this is enabling numerous exciting applications ranging from safe autonomous driving, to accurate face recognition, to automatic reading of radiology images.\n",
    "\n",
    "-- Andrew Ng\n",
    "```\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are responsible for the latest major breakthroughs in image recognition in the past few years.\n",
    "\n",
    "In mathematics, a convolution is a function that is applied over the output of another function. In our case, we will consider applying a matrix multiplication (filter) across an image. See the below diagram for an example of how this may work.\n",
    "\n",
    ":::{figure-md} 01_intro_cnn-dl\n",
    "<img src=\"../../images/deep-learning/CNN/01_intro_cnn.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of matrix mutliplication (filter) in CNN {cite}`reluwiki`\n",
    ":::\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<iframe src=\"../html/conv-demo/index.html\" width=\"105%\" height=\"700px;\" style=\"border:none;\"></iframe>\n",
    "A demo of convolution function. <a href=\"https://cs231n.github.io/convolutional-networks/\">[source]</a>\n",
    "</p>\n",
    "\n",
    "CNNs generally follow a structure. The main convolutional setup is (input array) -> (convolutional filter layer) -> (Pooling) -> (Activation layer). The above diagram depicts how a convolutional layer may create one feature. Generally, filters are multidimensional and end up creating many features. It is also common to have a completely separate filter-feature creator of different sizes acting on the same layer. After this convolutional filter, it is common to apply a pooling layer. This pooling may be a max-pooling or an average pooling or another aggregation. One of the key concepts here is that the pooling layer has no parameters while decreasing the layer size. See the below diagram for an example of max-pooling.\n",
    "\n",
    ":::{figure-md} 01_intro_cnn2-dl\n",
    "<img src=\"../../images/deep-learning/CNN/01_intro_cnn2.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Illustration of max pooling\n",
    ":::\n",
    "\n",
    "After the max pooling, there is generally an activation layer. One of the more common activation layers is the ReLU (Rectified Linear Unit) {cite}`reluwiki`.\n",
    "\n",
    "## MNIST handwritten digits\n",
    "\n",
    "Here we illustrate how to use a simple CNN with three convolutional units to predict the MNIST handwritten digits. \n",
    "\n",
    "```{note}\n",
    "There is good reason why this dataset is used like the 'hello world' of image recognition, it is fairly compact while having a decent amount of training, test, and validation data. It only has one channel (black and white) and only ten possible outputs (0-9).\n",
    "```\n",
    "\n",
    "When the script is done training the model, you should see similar output to the following graphs.\n",
    "\n",
    ":::{figure-md} 02_cnn1_loss_acc-dl\n",
    "<img src=\"../../images/deep-learning/CNN/02_cnn1_loss_acc.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Train MINIST dataset with CNN: accuracy and loss\n",
    ":::\n",
    "\n",
    "Training and test loss (left) and test batch accuracy (right).\n",
    "\n",
    ":::{figure-md} 02_cnn1_mnist_output-dl\n",
    "<img src=\"../../images/deep-learning/CNN/02_cnn1_mnist_output.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Train MINIST dataset with CNN: prediction output\n",
    ":::\n",
    "\n",
    "A random set of 6 digits with actual and predicted labels. You can see a prediction failure in the lower right box.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we will download the MNIST handwritten\n",
    "# digits and create a simple CNN network to predict the\n",
    "# digit category (0-9)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Load data\n",
    "data_dir = 'temp'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=False)\n",
    "\n",
    "# Convert images into 28x28 (they are downloaded as 1x784)\n",
    "train_xdata = np.array([np.reshape(x, (28, 28)) for x in mnist.train.images])\n",
    "test_xdata = np.array([np.reshape(x, (28, 28)) for x in mnist.test.images])\n",
    "\n",
    "# Convert labels into one-hot encoded vectors\n",
    "train_labels = mnist.train.labels\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "# Set model parameters\n",
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "evaluation_size = 500\n",
    "image_width = train_xdata[0].shape[0]\n",
    "image_height = train_xdata[0].shape[1]\n",
    "target_size = np.max(train_labels) + 1\n",
    "num_channels = 1  # greyscale = 1 channel\n",
    "generations = 500\n",
    "eval_every = 5\n",
    "conv1_features = 25\n",
    "conv2_features = 50\n",
    "max_pool_size1 = 2  # NxN window for 1st max pool layer\n",
    "max_pool_size2 = 2  # NxN window for 2nd max pool layer\n",
    "fully_connected_size1 = 100\n",
    "\n",
    "# Declare model placeholders\n",
    "x_input_shape = (batch_size, image_width, image_height, num_channels)\n",
    "x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "eval_input_shape = (evaluation_size, image_width, image_height, num_channels)\n",
    "eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "eval_target = tf.placeholder(tf.int32, shape=(evaluation_size))\n",
    "\n",
    "# Declare model parameters\n",
    "conv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "\n",
    "conv2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))\n",
    "\n",
    "# fully connected variables\n",
    "resulting_width = image_width // (max_pool_size1 * max_pool_size2)\n",
    "resulting_height = image_height // (max_pool_size1 * max_pool_size2)\n",
    "full1_input_size = resulting_width * resulting_height * conv2_features\n",
    "full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1],\n",
    "                           stddev=0.1, dtype=tf.float32))\n",
    "full1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "full2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n",
    "\n",
    "\n",
    "# Initialize Model Operations\n",
    "def my_conv_net(conv_input_data):\n",
    "    # First Conv-ReLU-MaxPool Layer\n",
    "    conv1 = tf.nn.conv2d(conv_input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],\n",
    "                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n",
    "\n",
    "    # Second Conv-ReLU-MaxPool Layer\n",
    "    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],\n",
    "                               strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')\n",
    "\n",
    "    # Transform Output into a 1xN layer for next fully connected layer\n",
    "    final_conv_shape = max_pool2.get_shape().as_list()\n",
    "    final_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]\n",
    "    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])\n",
    "\n",
    "    # First Fully Connected Layer\n",
    "    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "\n",
    "    # Second Fully Connected Layer\n",
    "    final_model_output = tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)\n",
    "    \n",
    "    return final_model_output\n",
    "\n",
    "model_output = my_conv_net(x_input)\n",
    "test_model_output = my_conv_net(eval_input)\n",
    "\n",
    "# Declare Loss Function (softmax cross entropy)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "\n",
    "# Create a prediction function\n",
    "prediction = tf.nn.softmax(model_output)\n",
    "test_prediction = tf.nn.softmax(test_model_output)\n",
    "\n",
    "\n",
    "# Create accuracy function\n",
    "def get_accuracy(logits, targets):\n",
    "    batch_predictions = np.argmax(logits, axis=1)\n",
    "    num_correct = np.sum(np.equal(batch_predictions, targets))\n",
    "    return 100. * num_correct/batch_predictions.shape[0]\n",
    "\n",
    "# Create an optimizer\n",
    "my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "# Initialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Start training loop\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for i in range(generations):\n",
    "    rand_index = np.random.choice(len(train_xdata), size=batch_size)\n",
    "    rand_x = train_xdata[rand_index]\n",
    "    rand_x = np.expand_dims(rand_x, 3)\n",
    "    rand_y = train_labels[rand_index]\n",
    "    train_dict = {x_input: rand_x, y_target: rand_y}\n",
    "    \n",
    "    sess.run(train_step, feed_dict=train_dict)\n",
    "    temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n",
    "    temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "    \n",
    "    if (i+1) % eval_every == 0:\n",
    "        eval_index = np.random.choice(len(test_xdata), size=evaluation_size)\n",
    "        eval_x = test_xdata[eval_index]\n",
    "        eval_x = np.expand_dims(eval_x, 3)\n",
    "        eval_y = test_labels[eval_index]\n",
    "        test_dict = {eval_input: eval_x, eval_target: eval_y}\n",
    "        test_preds = sess.run(test_prediction, feed_dict=test_dict)\n",
    "        temp_test_acc = get_accuracy(test_preds, eval_y)\n",
    "        \n",
    "        # Record and print results\n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_acc.append(temp_train_acc)\n",
    "        test_acc.append(temp_test_acc)\n",
    "        acc_and_loss = [(i+1), temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "        acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "    \n",
    "    \n",
    "# Matlotlib code to plot the loss and accuracies\n",
    "eval_indices = range(0, generations, eval_every)\n",
    "# Plot loss over time\n",
    "plt.plot(eval_indices, train_loss, 'k-')\n",
    "plt.title('Softmax Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot train and test accuracy\n",
    "plt.plot(eval_indices, train_acc, 'k-', label='Train Set Accuracy')\n",
    "plt.plot(eval_indices, test_acc, 'r--', label='Test Set Accuracy')\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot some samples\n",
    "# Plot the 6 of the last batch results:\n",
    "actuals = rand_y[0:6]\n",
    "predictions = np.argmax(temp_train_preds, axis=1)[0:6]\n",
    "images = np.squeeze(rand_x[0:6])\n",
    "\n",
    "Nrows = 2\n",
    "Ncols = 3\n",
    "for i in range(6):\n",
    "    plt.subplot(Nrows, Ncols, i+1)\n",
    "    plt.imshow(np.reshape(images[i], [28, 28]), cmap='Greys_r')\n",
    "    plt.title('Actual: ' + str(actuals[i]) + ' Pred: ' + str(predictions[i]),\n",
    "              fontsize=10)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14772c",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "```{seealso}\n",
    "Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. [CIFAR-10 and CIFAR-100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "```\n",
    "\n",
    "Here we will build a convolutional neural network to predict the `CIFAR-10` data.\n",
    "\n",
    "The script provided will download and unzip the `CIFAR-10` data. Then it will start training a CNN from scratch. You should see similar output at the end of the following two graphs.\n",
    "\n",
    ":::{figure-md} 03_cnn2_loss_acc-dl\n",
    "<img src=\"../../images/deep-learning/CNN/03_cnn2_loss_acc.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Train `CIFAR-10` dataset with CNN: accuracy and loss\n",
    ":::\n",
    "\n",
    "Here we see the training loss (left) and the test batch accuracy (right).\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we will download the CIFAR-10 images\n",
    "# and build a CNN model with dropout and regularization\n",
    "#\n",
    "# CIFAR is composed ot 50k train and 10k test\n",
    "# images that are 32x32.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Change Directory\n",
    "try:\n",
    "    abspath = os.path.abspath(__file__)\n",
    "except NameError:\n",
    "    abspath = os.getcwd()\n",
    "dname = os.path.dirname(abspath)\n",
    "os.chdir(dname)\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Set model parameters\n",
    "batch_size = 128\n",
    "data_dir = 'temp'\n",
    "output_every = 50\n",
    "generations = 20000\n",
    "eval_every = 500\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "crop_height = 24\n",
    "crop_width = 24\n",
    "num_channels = 3\n",
    "num_targets = 10\n",
    "extract_folder = 'cifar-10-batches-bin'\n",
    "\n",
    "# Exponential Learning Rate Decay Params\n",
    "learning_rate = 0.1\n",
    "lr_decay = 0.1\n",
    "num_gens_to_wait = 250.\n",
    "\n",
    "# Extract model parameters\n",
    "image_vec_length = image_height * image_width * num_channels\n",
    "record_length = 1 + image_vec_length # ( + 1 for the 0-9 label)\n",
    "\n",
    "# Load data\n",
    "data_dir = 'temp'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "cifar10_url = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "\n",
    "# Check if file exists, otherwise download it\n",
    "data_file = os.path.join(data_dir, 'cifar-10-binary.tar.gz')\n",
    "if os.path.isfile(data_file):\n",
    "    pass\n",
    "else:\n",
    "    # Download file\n",
    "    def progress(block_num, block_size, total_size):\n",
    "        progress_info = [cifar10_url, float(block_num * block_size) / float(total_size) * 100.0]\n",
    "        print('\\r Downloading {} - {:.2f}%'.format(*progress_info), end=\"\")\n",
    "    filepath, _ = urllib.request.urlretrieve(cifar10_url, data_file, progress)\n",
    "    # Extract file\n",
    "    tarfile.open(filepath, 'r:gz').extractall(data_dir)\n",
    "    \n",
    "\n",
    "# Define CIFAR reader\n",
    "def read_cifar_files(filename_queue, distort_images = True):\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_length)\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    record_bytes = tf.decode_raw(record_string, tf.uint8)\n",
    "    image_label = tf.cast(tf.slice(record_bytes, [0], [1]), tf.int32)\n",
    "  \n",
    "    # Extract image\n",
    "    image_extracted = tf.reshape(tf.slice(record_bytes, [1], [image_vec_length]),\n",
    "                                 [num_channels, image_height, image_width])\n",
    "    \n",
    "    # Reshape image\n",
    "    image_uint8image = tf.transpose(image_extracted, [1, 2, 0])\n",
    "    reshaped_image = tf.cast(image_uint8image, tf.float32)\n",
    "    # Randomly Crop image\n",
    "    final_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, crop_width, crop_height)\n",
    "    \n",
    "    if distort_images:\n",
    "        # Randomly flip the image horizontally, change the brightness and contrast\n",
    "        final_image = tf.image.random_flip_left_right(final_image)\n",
    "        final_image = tf.image.random_brightness(final_image,max_delta=63)\n",
    "        final_image = tf.image.random_contrast(final_image,lower=0.2, upper=1.8)\n",
    "\n",
    "    # Normalize whitening\n",
    "    final_image = tf.image.per_image_standardization(final_image)\n",
    "    return final_image, image_label\n",
    "\n",
    "\n",
    "# Create a CIFAR image pipeline from reader\n",
    "def input_pipeline(batch_size, train_logical=True):\n",
    "    if train_logical:\n",
    "        files = [os.path.join(data_dir, extract_folder, 'data_batch_{}.bin'.format(i)) for i in range(1,6)]\n",
    "    else:\n",
    "        files = [os.path.join(data_dir, extract_folder, 'test_batch.bin')]\n",
    "    filename_queue = tf.train.string_input_producer(files)\n",
    "    image, label = read_cifar_files(filename_queue)\n",
    "    \n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "    #   from -- bigger means better shuffling but slower start up and more\n",
    "    #   memory used.\n",
    "    # capacity must be larger than min_after_dequeue and the amount larger\n",
    "    #   determines the maximum we will prefetch.  Recommendation:\n",
    "    #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 5000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch([image, label],\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        capacity=capacity,\n",
    "                                                        min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    return example_batch, label_batch\n",
    "\n",
    "    \n",
    "# Define the model architecture, this will return logits from images\n",
    "def cifar_cnn_model(input_images, batch_size, train_logical=True):\n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return(tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.truncated_normal_initializer(stddev=0.05)))\n",
    "    def zero_var(name, shape, dtype):\n",
    "        return(tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.constant_initializer(0.0)))\n",
    "    \n",
    "    # First Convolutional Layer\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        # Conv_kernel is 5x5 for all 3 colors and we will create 64 features\n",
    "        conv1_kernel = truncated_normal_var(name='conv_kernel1', shape=[5, 5, 3, 64], dtype=tf.float32)\n",
    "        # We convolve across the image with a stride size of 1\n",
    "        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        # Initialize and add the bias term\n",
    "        conv1_bias = zero_var(name='conv_bias1', shape=[64], dtype=tf.float32)\n",
    "        conv1_add_bias = tf.nn.bias_add(conv1, conv1_bias)\n",
    "        # ReLU element wise\n",
    "        relu_conv1 = tf.nn.relu(conv1_add_bias)\n",
    "    \n",
    "    # Max Pooling\n",
    "    pool1 = tf.nn.max_pool(relu_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],padding='SAME', name='pool_layer1')\n",
    "    \n",
    "    # Local Response Normalization (parameters from paper)\n",
    "    # paper: http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
    "    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm1')\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        # Conv kernel is 5x5, across all prior 64 features and we create 64 more features\n",
    "        conv2_kernel = truncated_normal_var(name='conv_kernel2', shape=[5, 5, 64, 64], dtype=tf.float32)\n",
    "        # Convolve filter across prior output with stride size of 1\n",
    "        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        # Initialize and add the bias\n",
    "        conv2_bias = zero_var(name='conv_bias2', shape=[64], dtype=tf.float32)\n",
    "        conv2_add_bias = tf.nn.bias_add(conv2, conv2_bias)\n",
    "        # ReLU element wise\n",
    "        relu_conv2 = tf.nn.relu(conv2_add_bias)\n",
    "    \n",
    "    # Max Pooling\n",
    "    pool2 = tf.nn.max_pool(relu_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_layer2')    \n",
    "    \n",
    "     # Local Response Normalization (parameters from paper)\n",
    "    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm2')\n",
    "    \n",
    "    # Reshape output into a single matrix for multiplication for the fully connected layers\n",
    "    reshaped_output = tf.reshape(norm2, [batch_size, -1])\n",
    "    reshaped_dim = reshaped_output.get_shape()[1].value\n",
    "    \n",
    "    # First Fully Connected Layer\n",
    "    with tf.variable_scope('full1') as scope:\n",
    "        # Fully connected layer will have 384 outputs.\n",
    "        full_weight1 = truncated_normal_var(name='full_mult1', shape=[reshaped_dim, 384], dtype=tf.float32)\n",
    "        full_bias1 = zero_var(name='full_bias1', shape=[384], dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, full_weight1), full_bias1))\n",
    "\n",
    "    # Second Fully Connected Layer\n",
    "    with tf.variable_scope('full2') as scope:\n",
    "        # Second fully connected layer has 192 outputs.\n",
    "        full_weight2 = truncated_normal_var(name='full_mult2', shape=[384, 192], dtype=tf.float32)\n",
    "        full_bias2 = zero_var(name='full_bias2', shape=[192], dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, full_weight2), full_bias2))\n",
    "\n",
    "    # Final Fully Connected Layer -> 10 categories for output (num_targets)\n",
    "    with tf.variable_scope('full3') as scope:\n",
    "        # Final fully connected layer has 10 (num_targets) outputs.\n",
    "        full_weight3 = truncated_normal_var(name='full_mult3', shape=[192, num_targets], dtype=tf.float32)\n",
    "        full_bias3 =  zero_var(name='full_bias3', shape=[num_targets], dtype=tf.float32)\n",
    "        final_output = tf.add(tf.matmul(full_layer2, full_weight3), full_bias3)\n",
    "        \n",
    "    return final_output\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def cifar_loss(logits, targets):\n",
    "    # Get rid of extra dimensions and cast targets into integers\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    # Calculate cross entropy from logits and targets\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "    # Take the average loss across batch size\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    return cross_entropy_mean\n",
    "\n",
    "\n",
    "# Train step\n",
    "def train_step(loss_value, generation_num):\n",
    "    # Our learning rate is an exponential decay after we wait a fair number of generations\n",
    "    model_learning_rate = tf.train.exponential_decay(learning_rate, generation_num,\n",
    "                                                     num_gens_to_wait, lr_decay, staircase=True)\n",
    "    # Create optimizer\n",
    "    my_optimizer = tf.train.GradientDescentOptimizer(model_learning_rate)\n",
    "    # Initialize train step\n",
    "    train_step = my_optimizer.minimize(loss_value)\n",
    "    return train_step\n",
    "\n",
    "\n",
    "# Accuracy function\n",
    "def accuracy_of_batch(logits, targets):\n",
    "    # Make sure targets are integers and drop extra dimensions\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    # Get predicted values by finding which logit is the greatest\n",
    "    batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    # Check if they are equal across the batch\n",
    "    predicted_correctly = tf.equal(batch_predictions, targets)\n",
    "    # Average the 1's and 0's (True's and False's) across the batch size\n",
    "    accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "# Get data\n",
    "print('Getting/Transforming Data.')\n",
    "# Initialize the data pipeline\n",
    "images, targets = input_pipeline(batch_size, train_logical=True)\n",
    "# Get batch test images and targets from pipline\n",
    "test_images, test_targets = input_pipeline(batch_size, train_logical=False)\n",
    "\n",
    "# Declare Model\n",
    "print('Creating the CIFAR10 Model.')\n",
    "with tf.variable_scope('model_definition') as scope:\n",
    "    # Declare the training network model\n",
    "    model_output = cifar_cnn_model(images, batch_size)\n",
    "    # This is very important!!!  We must set the scope to REUSE the variables,\n",
    "    #  otherwise, when we set the test network model, it will create new random\n",
    "    #  variables.  Otherwise we get random evaluations on the test batches.\n",
    "    scope.reuse_variables()\n",
    "    test_output = cifar_cnn_model(test_images, batch_size)\n",
    "\n",
    "# Declare loss function\n",
    "print('Declare Loss Function.')\n",
    "loss = cifar_loss(model_output, targets)\n",
    "\n",
    "# Create accuracy function\n",
    "accuracy = accuracy_of_batch(test_output, test_targets)\n",
    "\n",
    "# Create training operations\n",
    "print('Creating the Training Operation.')\n",
    "generation_num = tf.Variable(0, trainable=False)\n",
    "train_op = train_step(loss, generation_num)\n",
    "\n",
    "# Initialize Variables\n",
    "print('Initializing the Variables.')\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Initialize queue (This queue will feed into the model, so no placeholders necessary)\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "# Train CIFAR Model\n",
    "print('Starting Training')\n",
    "train_loss = []\n",
    "test_accuracy = []\n",
    "for i in range(generations):\n",
    "    _, loss_value = sess.run([train_op, loss])\n",
    "    \n",
    "    if (i+1) % output_every == 0:\n",
    "        train_loss.append(loss_value)\n",
    "        output = 'Generation {}: Loss = {:.5f}'.format((i+1), loss_value)\n",
    "        print(output)\n",
    "    \n",
    "    if (i+1) % eval_every == 0:\n",
    "        [temp_accuracy] = sess.run([accuracy])\n",
    "        test_accuracy.append(temp_accuracy)\n",
    "        acc_output = ' --- Test Accuracy = {:.2f}%.'.format(100.*temp_accuracy)\n",
    "        print(acc_output)\n",
    "\n",
    "# Print loss and accuracy\n",
    "# Matlotlib code to plot the loss and accuracies\n",
    "eval_indices = range(0, generations, eval_every)\n",
    "output_indices = range(0, generations, output_every)\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(output_indices, train_loss, 'k-')\n",
    "plt.title('Softmax Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy over time\n",
    "plt.plot(eval_indices, test_accuracy, 'k-')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd698830",
   "metadata": {},
   "source": [
    "## How to fine-tune current CNN architectures?\n",
    "\n",
    "The purpose of the script provided in this section is to download the CIFAR-10 data and sort it out in the proper folder structure for running it through the TensorFlow fine-tuning tutorial. The script should create the following folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "-train_dir\n",
    "  |--airplane\n",
    "  |--automobile\n",
    "  |--bird\n",
    "  |--cat\n",
    "  |--deer\n",
    "  |--dog\n",
    "  |--frog\n",
    "  |--horse\n",
    "  |--ship\n",
    "  |--truck\n",
    "-validation_dir\n",
    "  |--airplane\n",
    "  |--automobile\n",
    "  |--bird\n",
    "  |--cat\n",
    "  |--deer\n",
    "  |--dog\n",
    "  |--frog\n",
    "  |--horse\n",
    "  |--ship\n",
    "  |--truck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced885e",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this script, we download the CIFAR-10 images and\n",
    "# transform/save them in the Inception Retraining Format\n",
    "#\n",
    "# The end purpose of the files is for re-training the\n",
    "# Google Inception tensorflow model to work on the CIFAR-10.\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import scipy.misc\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "cifar_link = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "data_dir = 'temp'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download tar file\n",
    "target_file = os.path.join(data_dir, 'cifar-10-python.tar.gz')\n",
    "if not os.path.isfile(target_file):\n",
    "    print('CIFAR-10 file not found. Downloading CIFAR data (Size = 163MB)')\n",
    "    print('This may take a few minutes, please wait.')\n",
    "    filename, headers = urllib.request.urlretrieve(cifar_link, target_file)\n",
    "\n",
    "# Extract into memory\n",
    "tar = tarfile.open(target_file)\n",
    "tar.extractall(path=data_dir)\n",
    "tar.close()\n",
    "objects = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Create train image folders\n",
    "train_folder = 'train_dir'\n",
    "if not os.path.isdir(os.path.join(data_dir, train_folder)):\n",
    "    for i in range(10):\n",
    "        folder = os.path.join(data_dir, train_folder, objects[i])\n",
    "        os.makedirs(folder)\n",
    "# Create test image folders\n",
    "test_folder = 'validation_dir'\n",
    "if not os.path.isdir(os.path.join(data_dir, test_folder)):\n",
    "    for i in range(10):\n",
    "        folder = os.path.join(data_dir, test_folder, objects[i])\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# Extract images accordingly\n",
    "data_location = os.path.join(data_dir, 'cifar-10-batches-py')\n",
    "train_names = ['data_batch_' + str(x) for x in range(1,6)]\n",
    "test_names = ['test_batch']\n",
    "\n",
    "\n",
    "def load_batch_from_file(file):\n",
    "    file_conn = open(file, 'rb')\n",
    "    image_dictionary = cPickle.load(file_conn, encoding='latin1')\n",
    "    file_conn.close()\n",
    "    return image_dictionary\n",
    "\n",
    "\n",
    "def save_images_from_dict(image_dict, folder='data_dir'):\n",
    "    # image_dict.keys() = 'labels', 'filenames', 'data', 'batch_label'\n",
    "    for ix, label in enumerate(image_dict['labels']):\n",
    "        folder_path = os.path.join(data_dir, folder, objects[label])\n",
    "        filename = image_dict['filenames'][ix]\n",
    "        #Transform image data\n",
    "        image_array = image_dict['data'][ix]\n",
    "        image_array.resize([3, 32, 32])\n",
    "        # Save image\n",
    "        output_location = os.path.join(folder_path, filename)\n",
    "        scipy.misc.imsave(output_location,image_array.transpose())\n",
    "\n",
    "# Sort train images\n",
    "for file in train_names:\n",
    "    print('Saving images from file: {}'.format(file))\n",
    "    file_location = os.path.join(data_dir, 'cifar-10-batches-py', file)\n",
    "    image_dict = load_batch_from_file(file_location)\n",
    "    save_images_from_dict(image_dict, folder=train_folder)\n",
    "\n",
    "# Sort test images\n",
    "for file in test_names:\n",
    "    print('Saving images from file: {}'.format(file))\n",
    "    file_location = os.path.join(data_dir, 'cifar-10-batches-py', file)\n",
    "    image_dict = load_batch_from_file(file_location)\n",
    "    save_images_from_dict(image_dict, folder=test_folder)\n",
    "    \n",
    "# Create labels file\n",
    "cifar_labels_file = os.path.join(data_dir,'cifar10_labels.txt')\n",
    "print('Writing labels file, {}'.format(cifar_labels_file))\n",
    "with open(cifar_labels_file, 'w') as labels_file:\n",
    "    for item in objects:\n",
    "        labels_file.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f460a",
   "metadata": {},
   "source": [
    "## Stylenet / Neural-Style\n",
    "\n",
    "The purpose of this script is to illustrate how to do stylenet in TensorFlow. We reference the following [paper](https://arxiv.org/abs/1508.06576) for this algorithm.\n",
    "\n",
    "But there is some prerequisites,\n",
    "\n",
    "- Download the `VGG-verydeep-19.mat` file.\n",
    "- You must download two images, a style image and a content image for the algorithm to blend.\n",
    "\n",
    "The style image is\n",
    "\n",
    ":::{figure-md} starry_night-dl\n",
    "<img src=\"../../images/deep-learning/CNN/starry_night.jpg\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Style image: starry night\n",
    ":::\n",
    "\n",
    "The context image is below.\n",
    "\n",
    ":::{figure-md} book_cover-dl\n",
    "<img src=\"../../images/deep-learning/CNN/book_cover.jpg\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Content image: book cover\n",
    ":::\n",
    "\n",
    "The final result looks like\n",
    "\n",
    ":::{figure-md} 05_stylenet_ex-dl\n",
    "<img src=\"../../images/deep-learning/CNN/05_stylenet_ex.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "stylenet final result\n",
    ":::\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use two images, an original image and a style image\n",
    "# and try to make the original image in the style of the style image.\n",
    "#\n",
    "# Reference paper:\n",
    "# https://arxiv.org/abs/1508.06576\n",
    "#\n",
    "# Need to download the model 'imagenet-vgg-verydee-19.mat' from:\n",
    "#   http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\n",
    "\n",
    "import os\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Image Files\n",
    "original_image_file = 'images/book_cover.jpg'\n",
    "style_image_file = 'images/starry_night.jpg'\n",
    "\n",
    "# Saved VGG Network path under the current project dir.\n",
    "vgg_path = 'imagenet-vgg-verydeep-19.mat'\n",
    "\n",
    "# Default Arguments\n",
    "original_image_weight = 5.0\n",
    "style_image_weight = 500.0\n",
    "regularization_weight = 100\n",
    "learning_rate = 10\n",
    "generations = 100\n",
    "output_generations = 25\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "# Read in images\n",
    "original_image = imageio.imread(original_image_file)\n",
    "style_image = imageio.imread(style_image_file)\n",
    "\n",
    "# Get shape of target and make the style image the same\n",
    "target_shape = original_image.shape\n",
    "style_image = resize(style_image, target_shape)\n",
    "\n",
    "# VGG-19 Layer Setup\n",
    "# From paper\n",
    "vgg_layers = ['conv1_1', 'relu1_1',\n",
    "              'conv1_2', 'relu1_2', 'pool1',\n",
    "              'conv2_1', 'relu2_1',\n",
    "              'conv2_2', 'relu2_2', 'pool2',\n",
    "              'conv3_1', 'relu3_1',\n",
    "              'conv3_2', 'relu3_2',\n",
    "              'conv3_3', 'relu3_3',\n",
    "              'conv3_4', 'relu3_4', 'pool3',\n",
    "              'conv4_1', 'relu4_1',\n",
    "              'conv4_2', 'relu4_2',\n",
    "              'conv4_3', 'relu4_3',\n",
    "              'conv4_4', 'relu4_4', 'pool4',\n",
    "              'conv5_1', 'relu5_1',\n",
    "              'conv5_2', 'relu5_2',\n",
    "              'conv5_3', 'relu5_3',\n",
    "              'conv5_4', 'relu5_4']\n",
    "\n",
    "\n",
    "# Extract weights and matrix means\n",
    "def extract_net_info(path_to_params):\n",
    "    vgg_data = scipy.io.loadmat(path_to_params)\n",
    "    normalization_matrix = vgg_data['normalization'][0][0][0]\n",
    "    mat_mean = np.mean(normalization_matrix, axis=(0,1))\n",
    "    network_weights = vgg_data['layers'][0]\n",
    "    return mat_mean, network_weights\n",
    "    \n",
    "\n",
    "# Create the VGG-19 Network\n",
    "def vgg_network(network_weights, init_image):\n",
    "    network = {}\n",
    "    image = init_image\n",
    "\n",
    "    for i, layer in enumerate(vgg_layers):\n",
    "        if layer[0] == 'c':\n",
    "            weights, bias = network_weights[i][0][0][0][0]\n",
    "            weights = np.transpose(weights, (1, 0, 2, 3))\n",
    "            bias = bias.reshape(-1)\n",
    "            conv_layer = tf.nn.conv2d(image, tf.constant(weights), (1, 1, 1, 1), 'SAME')\n",
    "            image = tf.nn.bias_add(conv_layer, bias)\n",
    "        elif layer[0] == 'r':\n",
    "            image = tf.nn.relu(image)\n",
    "        else:  # pooling\n",
    "            image = tf.nn.max_pool(image, (1, 2, 2, 1), (1, 2, 2, 1), 'SAME')\n",
    "        network[layer] = image\n",
    "    return network\n",
    "\n",
    "# Here we define which layers apply to the original or style image\n",
    "original_layers = ['relu4_2', 'relu5_2']\n",
    "style_layers = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n",
    "\n",
    "# Get network parameters\n",
    "normalization_mean, network_weights = extract_net_info(vgg_path)\n",
    "\n",
    "shape = (1,) + original_image.shape\n",
    "style_shape = (1,) + style_image.shape\n",
    "original_features = {}\n",
    "style_features = {}\n",
    "\n",
    "# Set style weights\n",
    "style_weights = {l: 1./(len(style_layers)) for l in style_layers}\n",
    "\n",
    "# Computer feature layers with original image\n",
    "g_original = tf.Graph()\n",
    "with g_original.as_default(), tf.Session() as sess1:\n",
    "    image = tf.placeholder('float', shape=shape)\n",
    "    vgg_net = vgg_network(network_weights, image)\n",
    "    original_minus_mean = original_image - normalization_mean\n",
    "    original_norm = np.array([original_minus_mean])\n",
    "    for layer in original_layers:\n",
    "        original_features[layer] = vgg_net[layer].eval(feed_dict={image: original_norm})\n",
    "\n",
    "# Get style image network\n",
    "g_style = tf.Graph()\n",
    "with g_style.as_default(), tf.Session() as sess2:\n",
    "    image = tf.placeholder('float', shape=style_shape)\n",
    "    vgg_net = vgg_network(network_weights, image)\n",
    "    style_minus_mean = style_image - normalization_mean\n",
    "    style_norm = np.array([style_minus_mean])\n",
    "    for layer in style_layers:\n",
    "        features = vgg_net[layer].eval(feed_dict={image: style_norm})\n",
    "        features = np.reshape(features, (-1, features.shape[3]))\n",
    "        gram = np.matmul(features.T, features) / features.size\n",
    "        style_features[layer] = gram\n",
    "\n",
    "# Make Combined Image via loss function\n",
    "with tf.Graph().as_default():\n",
    "    # Get network parameters\n",
    "    initial = tf.random_normal(shape) * 0.256\n",
    "    init_image = tf.Variable(initial)\n",
    "    vgg_net = vgg_network(network_weights, init_image)\n",
    "\n",
    "    # Loss from Original Image\n",
    "    original_layers_w = {'relu4_2': 0.5, 'relu5_2': 0.5}\n",
    "    original_loss = 0\n",
    "    for o_layer in original_layers:\n",
    "        temp_original_loss = original_layers_w[o_layer] * original_image_weight *\\\n",
    "                             (2 * tf.nn.l2_loss(vgg_net[o_layer] - original_features[o_layer]))\n",
    "        original_loss += (temp_original_loss / original_features[o_layer].size)\n",
    "\n",
    "    # Loss from Style Image\n",
    "    style_loss = 0\n",
    "    style_losses = []\n",
    "    for style_layer in style_layers:\n",
    "        layer = vgg_net[style_layer]\n",
    "        feats, height, width, channels = [x.value for x in layer.get_shape()]\n",
    "        size = height * width * channels\n",
    "        features = tf.reshape(layer, (-1, channels))\n",
    "        style_gram_matrix = tf.matmul(tf.transpose(features), features) / size\n",
    "        style_expected = style_features[style_layer]\n",
    "        style_losses.append(style_weights[style_layer] * 2 *\n",
    "                            tf.nn.l2_loss(style_gram_matrix - style_expected) /\n",
    "                            style_expected.size)\n",
    "    style_loss += style_image_weight * tf.reduce_sum(style_losses)\n",
    "\n",
    "    # To Smooth the results, we add in total variation loss\n",
    "    total_var_x = reduce(mul, init_image[:, 1:, :, :].get_shape().as_list(), 1)\n",
    "    total_var_y = reduce(mul, init_image[:, :, 1:, :].get_shape().as_list(), 1)\n",
    "    first_term = regularization_weight * 2\n",
    "    second_term_numerator = tf.nn.l2_loss(init_image[:, 1:, :, :] - init_image[:, :shape[1]-1, :, :])\n",
    "    second_term = second_term_numerator / total_var_y\n",
    "    third_term = (tf.nn.l2_loss(init_image[:, :, 1:, :] - init_image[:, :, :shape[2]-1, :]) / total_var_x)\n",
    "    total_variation_loss = first_term * (second_term + third_term)\n",
    "\n",
    "    # Combined Loss\n",
    "    loss = original_loss + style_loss + total_variation_loss\n",
    "\n",
    "    # Declare Optimization Algorithm\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and start training\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(generations):\n",
    "\n",
    "            train_step.run()\n",
    "\n",
    "            # Print update and save temporary output\n",
    "            if (i+1) % output_generations == 0:\n",
    "                print('Generation {} out of {}, loss: {}'.format(i + 1, generations,sess.run(loss)))\n",
    "                image_eval = init_image.eval()\n",
    "                best_image_add_mean = image_eval.reshape(shape[1:]) + normalization_mean\n",
    "                output_file = 'temp_output_{}.jpg'.format(i)\n",
    "                imageio.imwrite(output_file, best_image_add_mean)\n",
    "        \n",
    "        \n",
    "        # Save final image\n",
    "        image_eval = init_image.eval()\n",
    "        best_image_add_mean = image_eval.reshape(shape[1:]) + normalization_mean\n",
    "        output_file = 'final_output.jpg'\n",
    "        scipy.misc.imsave(output_file, best_image_add_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c6a2d",
   "metadata": {},
   "source": [
    "## Deepdream in TensorFlow\n",
    "Note: There is no new code in this script. It originates from the TensorFlow tutorial located here. However, this code is modified slightly to run on Python 3. The code is also commented very heavily to explain, line-by-line, what occurs in the deepdream demo.\n",
    "\n",
    "Here are some potential outputs.\n",
    "\n",
    ":::{figure-md} 06_deepdream_ex-dl\n",
    "<img src=\"../../images/deep-learning/CNN/06_deepdream_ex.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Deepdream outputs\n",
    ":::\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03524430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TensorFlow for Deep Dream\n",
    "#---------------------------------------\n",
    "# From: Alexander Mordvintsev\n",
    "#      --https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream\n",
    "#\n",
    "# Make sure to download the deep dream model here:\n",
    "#   https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\n",
    "#\n",
    "# Run:\n",
    "#  me@computer:~$ wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip \n",
    "#  me@computer:~$ unzip inception5h.zip\n",
    "#\n",
    "#  More comments added inline.\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "from io import BytesIO\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "os.chdir('~/Documents/tensorflow/inception-v1-model/')\n",
    "\n",
    "# Model filename\n",
    "model_fn = 'tensorflow_inception_graph.pb'\n",
    "\n",
    "# Load graph parameters\n",
    "with tf.gfile.FastGFile(model_fn, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "# Create placeholder for input\n",
    "t_input = tf.placeholder(np.float32, name='input')\n",
    "\n",
    "# Imagenet average bias to subtract off images\n",
    "imagenet_mean = 117.0\n",
    "t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\n",
    "tf.import_graph_def(graph_def, {'input':t_preprocessed})\n",
    "\n",
    "# Create a list of layers that we can refer to later\n",
    "layers = [op.name for op in graph.get_operations() if op.type=='Conv2D' and 'import/' in op.name]\n",
    "\n",
    "# Count how many outputs for each layer\n",
    "feature_nums = [int(graph.get_tensor_by_name(name+':0').get_shape()[-1]) for name in layers]\n",
    "\n",
    "# Print count of layers and outputs (features nodes)\n",
    "print('Number of layers', len(layers))\n",
    "print('Total number of feature channels:', sum(feature_nums))\n",
    "\n",
    "# Picking some internal layer. Note that we use outputs before applying the ReLU nonlinearity\n",
    "# to have non-zero gradients for features with negative initial activations.\n",
    "layer = 'mixed4d_3x3_bottleneck_pre_relu'\n",
    "channel = 30 # picking some feature channel to visualize\n",
    "\n",
    "# start with a gray image with a little noise\n",
    "img_noise = np.random.uniform(size=(224,224,3)) + 100.0\n",
    "\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    # First make sure everything is between 0 and 255\n",
    "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
    "    # Pick an in-memory format for image display\n",
    "    f = BytesIO()\n",
    "    # Create the in memory image\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    # Show image\n",
    "    plt.imshow(a)\n",
    "\n",
    "\n",
    "def T(layer):\n",
    "    '''Helper for getting layer output tensor'''\n",
    "    return graph.get_tensor_by_name(\"import/%s:0\"%layer)\n",
    "\n",
    "\n",
    "# The following function returns a function wrapper that will create the placeholder\n",
    "# inputs of a specified dtype\n",
    "def tffunc(*argtypes):\n",
    "    '''Helper that transforms TF-graph generating function into a regular one.\n",
    "    See \"resize\" function below.\n",
    "    '''\n",
    "    placeholders = list(map(tf.placeholder, argtypes))\n",
    "    def wrap(f):\n",
    "        out = f(*placeholders)\n",
    "        def wrapper(*args, **kw):\n",
    "            return out.eval(dict(zip(placeholders, args)), session=kw.get('session'))\n",
    "        return wrapper\n",
    "    return wrap\n",
    "\n",
    "\n",
    "# Helper function that uses TF to resize an image\n",
    "def resize(img, size):\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    # Change 'img' size by linear interpolation\n",
    "    return tf.image.resize_bilinear(img, size)[0, :, :, :]\n",
    "\n",
    "\n",
    "def calc_grad_tiled(img, t_grad, tile_size=512):\n",
    "    '''Compute the value of tensor t_grad over the image in a tiled way.\n",
    "    Random shifts are applied to the image to blur tile boundaries over \n",
    "    multiple iterations.'''\n",
    "    # Pick a subregion square size\n",
    "    sz = tile_size\n",
    "    # Get the image height and width\n",
    "    h, w = img.shape[:2]\n",
    "    # Get a random shift amount in the x and y direction\n",
    "    sx, sy = np.random.randint(sz, size=2)\n",
    "    # Randomly shift the image (roll image) in the x and y directions\n",
    "    img_shift = np.roll(np.roll(img, sx, 1), sy, 0)\n",
    "    # Initialize the while image gradient as zeros\n",
    "    grad = np.zeros_like(img)\n",
    "    # Now we loop through all the sub-tiles in the image\n",
    "    for y in range(0, max(h-sz//2, sz),sz):\n",
    "        for x in range(0, max(w-sz//2, sz),sz):\n",
    "            # Select the sub image tile\n",
    "            sub = img_shift[y:y+sz,x:x+sz]\n",
    "            # Calculate the gradient for the tile\n",
    "            g = sess.run(t_grad, {t_input:sub})\n",
    "            # Apply the gradient of the tile to the whole image gradient\n",
    "            grad[y:y+sz,x:x+sz] = g\n",
    "    # Return the gradient, undoing the roll operation\n",
    "    return np.roll(np.roll(grad, -sx, 1), -sy, 0)\n",
    "\n",
    "def render_deepdream(t_obj, img0=img_noise,\n",
    "                     iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):\n",
    "    # defining the optimization objective, the objective is the mean of the feature\n",
    "    t_score = tf.reduce_mean(t_obj)\n",
    "    # Our gradients will be defined as changing the t_input to get closer to\n",
    "    # the values of t_score.  Here, t_score is the mean of the feature we select,\n",
    "    # and t_input will be the image octave (starting with the last)\n",
    "    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n",
    "\n",
    "    # Store the image\n",
    "    img = img0\n",
    "    # Initialize the octave list\n",
    "    octaves = []\n",
    "    # Since we stored the image, we need to only calculate n-1 octaves\n",
    "    for i in range(octave_n-1):\n",
    "        # Extract the image shape\n",
    "        hw = img.shape[:2]\n",
    "        # Resize the image, scale by the octave_scale (resize by linear interpolation)\n",
    "        lo = resize(img, np.int32(np.float32(hw)/octave_scale))\n",
    "        # Residual is hi.  Where residual = image - (Resize lo to be hw-shape)\n",
    "        hi = img-resize(lo, hw)\n",
    "        # Save the lo image for re-iterating\n",
    "        img = lo\n",
    "        # Save the extracted hi-image\n",
    "        octaves.append(hi)\n",
    "    \n",
    "    # generate details octave by octave\n",
    "    for octave in range(octave_n):\n",
    "        if octave>0:\n",
    "            # Start with the last octave\n",
    "            hi = octaves[-octave]\n",
    "            #\n",
    "            img = resize(img, hi.shape[:2])+hi\n",
    "        for i in range(iter_n):\n",
    "            # Calculate gradient of the image.\n",
    "            g = calc_grad_tiled(img, t_grad)\n",
    "            # Ideally, we would just add the gradient, g, but\n",
    "            # we want do a forward step size of it ('step'),\n",
    "            # and divide it by the avg. norm of the gradient, so\n",
    "            # we are adding a gradient of a certain size each step.\n",
    "            # Also, to make sure we aren't dividing by zero, we add 1e-7.\n",
    "            img += g*(step / (np.abs(g).mean()+1e-7))\n",
    "            print('.',end = ' ')\n",
    "        showarray(img/255.0)\n",
    "\n",
    "# Run Deep Dream\n",
    "if __name__==\"__main__\":\n",
    "    # Create resize function that has a wrapper that creates specified placeholder types\n",
    "    resize = tffunc(np.float32, np.int32)(resize)\n",
    "    \n",
    "    # Open image\n",
    "    img0 = PIL.Image.open('book_cover.jpg')\n",
    "    img0 = np.float32(img0)\n",
    "    # Show Original Image\n",
    "    showarray(img0/255.0)\n",
    "\n",
    "    # Create deep dream\n",
    "    render_deepdream(T(layer)[:, :, :, channel], img0, iter_n=15)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e63a77",
   "metadata": {},
   "source": [
    "## Your turn! 🚀\n",
    "\n",
    "TBD.\n",
    "\n",
    "## Self study\n",
    "\n",
    "You can refer to those YouTube videos for further study:\n",
    "\n",
    "- [Convolutional Neural Networks (CNNs) explained, by deeplizard](https://www.youtube.com/watch?v=YRhxdVk_sIs)\n",
    "- [Convolutional Neural Networks Explained (CNN Visualized), by Futurology](https://www.youtube.com/watch?v=pj9-rr1wDhM)\n",
    "\n",
    "### Research trend\n",
    "\n",
    "State of the Art Convolutional Neural Networks (CNNs) Explained | Deep Learning in 2020:\n",
    "\n",
    "<div class=\"yt-container\">\n",
    "   <iframe src=\"https://www.youtube.com/embed/YUyec4eCEiY\" allowfullscreen></iframe>\n",
    "</div>\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Nick](https://github.com/nfmcclure) for creating the open-source course [tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook). It inspires the majority of the content in this chapter.\n",
    "\n",
    "---\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   78,
   269,
   291,
   596,
   602,
   625,
   629,
   721,
   758,
   958,
   973,
   1166
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
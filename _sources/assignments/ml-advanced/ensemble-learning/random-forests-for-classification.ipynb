{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5ba4fe",
   "metadata": {},
   "source": [
    "# Random forests for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07494ffb",
   "metadata": {},
   "source": [
    "Random Forests are powerful machine learning algorithms used for supervised classification and regression. Random forests works by averaging the predictions of the multiple and randomized decision trees. Decision trees tends to overfit and so by combining multiple decision trees, the effect of overfitting can be minimized. \n",
    "\n",
    "Random Forests are type of ensemble models. More about ensembles models in the next assignment. \n",
    "\n",
    "Different to other learning algorithms, random forests provide a way to find the importance of each feature and this is implemented in Sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99058d12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df278c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35699c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "In this assignment, we will use Random forests to build a classifier that identify the increase or decrease of the electricity using \"the data that was collected from the Australian New South Wales Electricity Market. In this market, prices are not fixed and are affected by demand and supply of the market. They are set every five minutes. Electricity transfers to/from the neighboring state of Victoria were done to alleviate fluctuations.\"\n",
    "\n",
    "\"The dataset contains 45,312 instances dated from 7 May 1996 to 5 December 1998. Each example of the dataset refers to a period of 30 minutes, i.e. there are 48 instances for each time period of one day. Each example on the dataset has 5 fields, the day of week, the time stamp, the New South Wales electricity demand, the Victoria electricity demand, the scheduled electricity transfer between states and the class label. The class label identifies the change of the price (UP or DOWN) in New South Wales relative to a moving average of the last 24 hours (and removes the impact of longer term price trends). Source: [Open ML electricity](https://www.openml.org/d/151).\n",
    "\n",
    "Here are the information about the features:\n",
    "\n",
    "* Date: date between 7 May 1996 to 5 December 1998. Here normalized between 0 and 1 \n",
    "* Day: day of the week (1-7) \n",
    "* Period: time of the measurement (1-48) in half hour intervals over 24 hours. Here normalized between 0 and 1 \n",
    "* NSWprice: New South Wales electricity price, normalized between 0 and 1 \n",
    "* NSWdemand: New South Wales electricity demand, normalized between 0 and 1 \n",
    "* VICprice: Victoria electricity price, normalized between 0 and 1 \n",
    "* VICdemand: Victoria electricity demand, normalized between 0 and 1 \n",
    "* transfer: scheduled electricity transfer between both states, normalized between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67905ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's hide warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3083946",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df = pd.read_csv(\"../../../assets/data/elec_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06926d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(elec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ed67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a92c18-8bbe-4c82-8e1a-c4fa7a840bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492fd93",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 1: Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9906083",
   "metadata": {},
   "source": [
    "Before doing exploratory analysis, as always, let's split the data into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d74144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(elec_df, test_size=0.25, random_state=20)\n",
    "\n",
    "print(\n",
    "    \"The size of training data is: {} \\nThe size of testing data is: {}\".format(\n",
    "        len(train_data), len(test_data)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c895bd5e",
   "metadata": {},
   "source": [
    "Taking a quick look into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the last rows\n",
    "\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc96380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891be01",
   "metadata": {},
   "source": [
    "Two things to draw from the dataset for now:\n",
    "\n",
    "* The target feature `class` is categorical. We will make sure to encode that during data preprocessing. \n",
    "* All numerical features are already normalized, so we won't need to normalize these type of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753bb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking summary statistics\n",
    "\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values\n",
    "\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977a1a1",
   "metadata": {},
   "source": [
    "Great, we don't have any missing values. Usually there are three things to do with if they are present:\n",
    "* We can remove all missing values completely\n",
    "* We can leave them as they are\n",
    "* We can fill them with a given strategy such as mean, media or most frequent value. Either `Sklearn` or Pandas provides a quick ways to fill these kind of values. \n",
    "\n",
    "If you still want to know more about how to deal with missing values, please refer to [this article](https://medium.com/analytics-vidhya/a-comprehensive-guide-for-handling-missing-values-990c999c49ed?source=---------11----------------------------)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking feature correlation\n",
    "\n",
    "corr = train_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing correlation\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "sns.heatmap(corr, annot=True, cmap=\"crest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba4e088",
   "metadata": {},
   "source": [
    "It seems that we don't have features which are too correlating. Correlation shown above varies from `-1 to 1`. If the correlation between two features is close to 1, it means that they nearly contain the same information. If it is close to -1, it means that these features contain different information.Take an example: `vicdemand` correlate with `nswdeman` at 0.67 ratio. \n",
    "\n",
    "So if you drop one of those features, it's likely that your model will not be affected much. So different to what you have seen in many articles, having features which does not correlate to the target feature doesn't mean they are not useful. \n",
    "\n",
    "In the above correlation matrix, you can see that class feature is not there and this is because it still has categorical values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0fe21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 2: More data exploration\n",
    "\n",
    "Before preprocessing the data, let's take a look into specific features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd0ab0",
   "metadata": {},
   "source": [
    "Let's see how many Ups/Downs are in the class feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.countplot(data=train_data, x=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d0b62",
   "metadata": {},
   "source": [
    "`Day` is the days of the week, from 1-7, Monday to Sunday. Let's count the days occurences in respect to the ups/downs of the electricity's price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 8))\n",
    "\n",
    "sns.countplot(data=train_data, x=\"day\", hue=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0131d6",
   "metadata": {},
   "source": [
    "It seems that most days had downs. From the beginning of the week, there were consistent increase in downs(price of electricity went down) and decrease in ups.\n",
    "Let's see if there is an appealing relationship between the demand/price of electricity in New South Wales and Victoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c35605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 8))\n",
    "sns.scatterplot(data=train_data, x=\"vicdemand\", y=\"nswdemand\", hue=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8607e6",
   "metadata": {},
   "source": [
    "The demand of the electricity in New South Wales and the Victoria is kind of linear. Let's see if we can get any other insights by bringing days in the demand analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(data=train_data, x=\"vicdemand\", y=\"nswdemand\", hue=\"day\", size=\"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c2967",
   "metadata": {},
   "source": [
    "Although it is kind of hard to draw a strong point, there is less demand of electricity in both cities on Monday and Sunday than other days. We can use a line plot to plot the demand in both cities on the course of the days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2af8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 8))\n",
    "sns.lineplot(data=train_data, x=\"day\", y=\"nswdemand\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 8))\n",
    "sns.lineplot(data=train_data, x=\"day\", y=\"vicdemand\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c567dc3",
   "metadata": {},
   "source": [
    "Another interesting thing to look in the dataset is if there are some seasonalities/trends in the demand/price in either Victoria or New South Wales over period of time. In time series analysis, seasonality is when there is repetitive scenarios or consistent behaviours over the course of time. \n",
    "\n",
    "If you look at the demand of the electricity in both cities on the course of date (`7 May 1996 to 5 December 1998`), you can see that there are some types of seasonalities. Not 100% but it seems there is and if this dataset would have been collected for more than two years, it would probably be easy to know surely if there are seasonalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.lineplot(data=train_data, x=\"date\", y=\"nswdemand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee42b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.lineplot(data=train_data, x=\"date\", y=\"vicdemand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5babb8",
   "metadata": {},
   "source": [
    "One last thing about data analysis, let's plot all histograms of the numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.hist(bins=50, figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6733727",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3: Data preprocessing \n",
    "\n",
    "It is here that we prepare the data to be in the proper format for the machine learning model. \n",
    "\n",
    "Let's encode the categorical feature `class`. But before that, let's take training input data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"class\", axis=1)\n",
    "y_train = train_data[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "y_train_prepared = label_enc.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59a2f9-1121-474e-9c9d-b7a51a430b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cab705-7503-4213-bf38-42069cf2d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134d334",
   "metadata": {},
   "source": [
    "Now we are ready to train the machine learning model. \n",
    "\n",
    "But again if you look at the data, the `day` feature is not normalized as other features. We can normalize it or leave it but for now let's go ahead and train the random forests classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd496de6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4: Training random forests classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(\n",
    "    min_samples_split=2,\n",
    "    bootstrap=False,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_features=\"sqrt\",\n",
    ")\n",
    "\n",
    "forest_clf.fit(X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb9bde",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task5: Evaluating random forests classifier\n",
    "\n",
    "Let's build 3 functions to display accuracy, confusion matrix, and classification report. \n",
    "\n",
    "* Accuracy provide a percentage score of the model's ability to make correct predictions.\n",
    "* Confusion matrix shows the predicted classes and the actual classes: True Negativse(TN), True Positives(TP), False Negatives(FN), and True Positives(TP). \n",
    "* Classification report contains all useful metrics such as precision, recall, and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def accuracy(input_data, model, labels):\n",
    "    \"\"\"\n",
    "    Take the input data, model and labels and return accuracy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    preds = model.predict(input_data)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad946c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def conf_matrix(input_data, model, labels):\n",
    "    \"\"\"\n",
    "    Take the input data, model and labels and return confusion matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    preds = model.predict(input_data)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f19cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def class_report(input_data, model, labels):\n",
    "    \"\"\"\n",
    "    Take the input data, model and labels and return confusion matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    preds = model.predict(input_data)\n",
    "    report = classification_report(labels, preds)\n",
    "    report = print(report)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a052ed6",
   "metadata": {},
   "source": [
    "Let's find the accuracy on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17fed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(X_train, forest_clf, y_train_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeda399",
   "metadata": {},
   "source": [
    "Ohh, the model overfitted the dataset. Let's also display the classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b71a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(X_train, forest_clf, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report(X_train, forest_clf, y_train_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6022d",
   "metadata": {},
   "source": [
    "The model clearly overfitted the data. Let's see how we can regularize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a574a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 6: Improving random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model parameters\n",
    "\n",
    "forest_clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af110f75",
   "metadata": {},
   "source": [
    "We will use GridSearch to find the best hyperparameters that we can use to retrain the model with. By setting the `refit` to `True`, the random forest will be automatically retrained on the dataset with the best hyperparameters. By default, `refit` is True.\n",
    "\n",
    "We will also provide set `class_weight` to `balanced` since the data is not balanced. By doing that, the model will update the class weight automatically based off the number of examples available in each class. \n",
    "\n",
    "But this step takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056818ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],\n",
    "    \"max_leaf_nodes\": list(range(2, 12)),\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "# refit is true by default. The best estimator is trained on the whole dataset\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(\n",
    "        bootstrap=False, class_weight=\"balanced\", n_jobs=-1, max_features=\"sqrt\"\n",
    "    ),\n",
    "    params_grid,\n",
    "    verbose=1,\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc64ef",
   "metadata": {},
   "source": [
    "Let's find the accuracy again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(X_train, forest_best, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bf46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(X_train, forest_best, y_train_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d757301",
   "metadata": {},
   "source": [
    "In confusion matrix, each row represent an actual class and each column represents predicted class.\n",
    "\n",
    "So, from the results above:\n",
    "    \n",
    "* 16928 negative examples(N) were correcty predicted as negatives(true negatives).\n",
    "* 2613 negatives examples(N) were incorrectly classified as positive examples when they are in fact negatives(false positives).\n",
    "* 4223 positive examples were incorrectly classified as negative(N) when in fact they are positives(P) (false negatives).\n",
    "* 10220 were correctly classified as positive examples(true positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report(X_train, forest_best, y_train_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50f101",
   "metadata": {},
   "source": [
    "Wow, not so impressive, but this is much better than the first model. By only setting the class weight to balanced and finding the best values of the hyperparameters, we were able to improve our model.\n",
    "\n",
    "If you remember, we have classes imbalance. You can see the number of examples in each class in support in classification report. But our model is able to identify negative examples correctly at 80%, and also is able to identify the positive examples at 80% without overfitting. That is precision.\n",
    "\n",
    "A few notes about Precison/Recall/F1 score:\n",
    "\n",
    "* Precision is the model accuracy on predicting positive examples correctly.\n",
    "* Recall is the ratio of the positive examples that are correctly identified by the model.\n",
    "* F1 score is the harmonic mean of precision and recall.\n",
    "\n",
    "The higher the precision and recall are, the higher the F1 score. But there is a tradeoff between them. Increasing precision will reduce recall, and vice versa. So it's fair to say that it depends on the problem you're trying to solve and the metrics you want to optimize for. \n",
    "\n",
    "One way to improve the model can be to search more hyperparameters or adding more good data is always the best cure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc246af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 7: Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f82117",
   "metadata": {},
   "source": [
    "Let us evaluate the model on the test set. But we need first run the label_encoder on the class feature as we did in the training labels. Note that we only transform (not fit_transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop(\"class\", axis=1)\n",
    "y_test = test_data[\"class\"]\n",
    "\n",
    "y_test_prepared = label_enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cc7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(X_test, forest_best, y_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fcfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(X_test, forest_best, y_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67633d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report(X_test, forest_best, y_test_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e021a",
   "metadata": {},
   "source": [
    "As you can see the model is no longer overfitting. On the training set, the accuracy was 79%, which is a figure very similar to the test set. And the model never saw the test data. To improve the model in the case like this, if is often best to add more data if possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3766652f",
   "metadata": {},
   "source": [
    "### Task 8: Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b5f8c",
   "metadata": {},
   "source": [
    "Different to other machine learning models, random forests can show how each feature contributed to the model generalization. Let's find it.\n",
    "\n",
    "The results are values between 0 and 1. The closer to 1, the good the feature was to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee98d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import = forest_best.feature_importances_\n",
    "\n",
    "feat_df = pd.DataFrame(\n",
    "    feat_import, columns=[\"Feature Importance\"], index=X_train.columns\n",
    ")\n",
    "feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613450c3",
   "metadata": {},
   "source": [
    "From the dataframe above, the price of electricity in New South Wales had the top importance on the prediction of the electricity's cost fluctuation(Up/Down). Other features which highly influenced the model are demands in both South Wales and Victoria. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af266a08",
   "metadata": {},
   "source": [
    "This is the end of the assignment. We have learned the fundamental idea behind the random forests, how to overcome overfitting and how to find the feature importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af5728",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Thanks to Nyandwi for creating the open-source course [Machine Learning complete](https://github.com/Nyandwi/machine_learning_complete). It inspires the majority of the content in this chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
